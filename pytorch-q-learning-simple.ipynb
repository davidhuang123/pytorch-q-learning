{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Agent; reload(Agent)\n",
    "import Estimator; reload(Estimator)\n",
    "\n",
    "from Agent import Agent\n",
    "\n",
    "from Estimator import LinearEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent = Agent(2,cart_pole=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-11 10:44:33,996] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "_env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Environment import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = Environment(_env,1,False,[0,1],False,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop at: Tue Jul 11 10:45:21 2017\n",
      "populating replay buffer... \n",
      "replay buffer populated with 10000 transitions, learning begins...\n",
      "synchronizing target estimator !\n",
      "saving current_model\n",
      "\n",
      "\n",
      "global step: 177\n",
      "episode: 10\n",
      "running reward: 11.238994852800003\n",
      "current epsilon: 0.99648352\n",
      "episode_length: 15\n",
      "episode reward: 15.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 365\n",
      "episode: 20\n",
      "running reward: 16.252678689666233\n",
      "current epsilon: 0.99272728\n",
      "episode_length: 24\n",
      "episode reward: 24.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 584\n",
      "episode: 30\n",
      "running reward: 19.14648093835934\n",
      "current epsilon: 0.98835166\n",
      "episode_length: 16\n",
      "episode reward: 16.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 806\n",
      "episode: 40\n",
      "running reward: 21.56797103959152\n",
      "current epsilon: 0.9839161\n",
      "episode_length: 13\n",
      "episode reward: 13.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 958\n",
      "episode: 50\n",
      "running reward: 17.91630030500675\n",
      "current epsilon: 0.98087914\n",
      "episode_length: 18\n",
      "episode reward: 18.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 1149\n",
      "episode: 60\n",
      "running reward: 18.57782301061291\n",
      "current epsilon: 0.97706296\n",
      "episode_length: 19\n",
      "episode reward: 19.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 1395\n",
      "episode: 70\n",
      "running reward: 20.890461238394398\n",
      "current epsilon: 0.97214788\n",
      "episode_length: 26\n",
      "episode reward: 26.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 1582\n",
      "episode: 80\n",
      "running reward: 19.511408004172875\n",
      "current epsilon: 0.96841162\n",
      "episode_length: 13\n",
      "episode reward: 13.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 1773\n",
      "episode: 90\n",
      "running reward: 19.409187884649654\n",
      "current epsilon: 0.96459544\n",
      "episode_length: 20\n",
      "episode reward: 20.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 1956\n",
      "episode: 100\n",
      "running reward: 19.11395616312746\n",
      "current epsilon: 0.9609391\n",
      "episode_length: 16\n",
      "episode reward: 16.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 2244\n",
      "episode: 110\n",
      "running reward: 25.270039270999067\n",
      "current epsilon: 0.95518486\n",
      "episode_length: 16\n",
      "episode reward: 16.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 2453\n",
      "episode: 120\n",
      "running reward: 22.577938217577696\n",
      "current epsilon: 0.95100904\n",
      "episode_length: 28\n",
      "episode reward: 28.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 2631\n",
      "episode: 130\n",
      "running reward: 18.940474106879172\n",
      "current epsilon: 0.9474526\n",
      "episode_length: 18\n",
      "episode reward: 18.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 2947\n",
      "episode: 140\n",
      "running reward: 28.373333470741077\n",
      "current epsilon: 0.94113892\n",
      "episode_length: 11\n",
      "episode reward: 11.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 3167\n",
      "episode: 150\n",
      "running reward: 25.63189413501512\n",
      "current epsilon: 0.93674332\n",
      "episode_length: 40\n",
      "episode reward: 40.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 3439\n",
      "episode: 160\n",
      "running reward: 25.294528961705417\n",
      "current epsilon: 0.93130876\n",
      "episode_length: 17\n",
      "episode reward: 17.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 3616\n",
      "episode: 170\n",
      "running reward: 20.251368945931723\n",
      "current epsilon: 0.9277723\n",
      "episode_length: 15\n",
      "episode reward: 15.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 3828\n",
      "episode: 180\n",
      "running reward: 22.74700976095706\n",
      "current epsilon: 0.92353654\n",
      "episode_length: 45\n",
      "episode reward: 45.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 4013\n",
      "episode: 190\n",
      "running reward: 20.415420831489985\n",
      "current epsilon: 0.91984024\n",
      "episode_length: 27\n",
      "episode reward: 27.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 4202\n",
      "episode: 200\n",
      "running reward: 19.000880891908977\n",
      "current epsilon: 0.91606402\n",
      "episode_length: 11\n",
      "episode reward: 11.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 4483\n",
      "episode: 210\n",
      "running reward: 23.667470219716723\n",
      "current epsilon: 0.91044964\n",
      "episode_length: 31\n",
      "episode reward: 31.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 4757\n",
      "episode: 220\n",
      "running reward: 26.180011377624037\n",
      "current epsilon: 0.90497512\n",
      "episode_length: 43\n",
      "episode reward: 43.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 4995\n",
      "episode: 230\n",
      "running reward: 24.730967022350207\n",
      "current epsilon: 0.90021988\n",
      "episode_length: 19\n",
      "episode reward: 19.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 5207\n",
      "episode: 240\n",
      "running reward: 22.322640533817616\n",
      "current epsilon: 0.89598412\n",
      "episode_length: 20\n",
      "episode reward: 20.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 5427\n",
      "episode: 250\n",
      "running reward: 22.54355425614456\n",
      "current epsilon: 0.89158852\n",
      "episode_length: 23\n",
      "episode reward: 23.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 5721\n",
      "episode: 260\n",
      "running reward: 27.1567472757422\n",
      "current epsilon: 0.8857144\n",
      "episode_length: 27\n",
      "episode reward: 27.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 5881\n",
      "episode: 270\n",
      "running reward: 19.443236323795727\n",
      "current epsilon: 0.8825176\n",
      "episode_length: 10\n",
      "episode reward: 10.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 6091\n",
      "episode: 280\n",
      "running reward: 20.728599853276762\n",
      "current epsilon: 0.8783218\n",
      "episode_length: 31\n",
      "episode reward: 31.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 6301\n",
      "episode: 290\n",
      "running reward: 20.795289163697635\n",
      "current epsilon: 0.874126\n",
      "episode_length: 28\n",
      "episode reward: 28.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 6514\n",
      "episode: 300\n",
      "running reward: 20.866422334426534\n",
      "current epsilon: 0.86987026\n",
      "episode_length: 11\n",
      "episode reward: 11.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 6731\n",
      "episode: 310\n",
      "running reward: 20.744098365035647\n",
      "current epsilon: 0.8655346\n",
      "episode_length: 17\n",
      "episode reward: 17.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 6985\n",
      "episode: 320\n",
      "running reward: 23.282668850301597\n",
      "current epsilon: 0.86045968\n",
      "episode_length: 13\n",
      "episode reward: 13.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 7319\n",
      "episode: 330\n",
      "running reward: 30.369208970088025\n",
      "current epsilon: 0.85378636\n",
      "episode_length: 73\n",
      "episode reward: 73.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 7522\n",
      "episode: 340\n",
      "running reward: 23.264968091161226\n",
      "current epsilon: 0.84973042\n",
      "episode_length: 27\n",
      "episode reward: 27.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 7764\n",
      "episode: 350\n",
      "running reward: 25.070073880902378\n",
      "current epsilon: 0.8448952599999999\n",
      "episode_length: 27\n",
      "episode reward: 27.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 8040\n",
      "episode: 360\n",
      "running reward: 25.198545989084796\n",
      "current epsilon: 0.8393807799999999\n",
      "episode_length: 10\n",
      "episode reward: 10.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 8212\n",
      "episode: 370\n",
      "running reward: 19.481360169562205\n",
      "current epsilon: 0.83594422\n",
      "episode_length: 16\n",
      "episode reward: 16.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 8486\n",
      "episode: 380\n",
      "running reward: 24.27623054414923\n",
      "current epsilon: 0.8304697\n",
      "episode_length: 29\n",
      "episode reward: 29.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 8794\n",
      "episode: 390\n",
      "running reward: 30.462199354641932\n",
      "current epsilon: 0.82431586\n",
      "episode_length: 84\n",
      "episode reward: 84.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 9015\n",
      "episode: 400\n",
      "running reward: 25.07773151059178\n",
      "current epsilon: 0.81990028\n",
      "episode_length: 26\n",
      "episode reward: 26.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 9229\n",
      "episode: 410\n",
      "running reward: 23.625743218059764\n",
      "current epsilon: 0.81562456\n",
      "episode_length: 36\n",
      "episode reward: 36.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 9468\n",
      "episode: 420\n",
      "running reward: 24.565516466976245\n",
      "current epsilon: 0.81084934\n",
      "episode_length: 28\n",
      "episode reward: 28.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 9696\n",
      "episode: 430\n",
      "running reward: 23.36190016555615\n",
      "current epsilon: 0.8062939\n",
      "episode_length: 22\n",
      "episode reward: 22.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 9957\n",
      "episode: 440\n",
      "running reward: 25.589466261098053\n",
      "current epsilon: 0.80107912\n",
      "episode_length: 45\n",
      "episode reward: 45.0\n",
      "\n",
      "\n",
      "synchronizing target estimator !\n",
      "saving current_model\n",
      "\n",
      "\n",
      "global step: 10214\n",
      "episode: 450\n",
      "running reward: 24.42540232731125\n",
      "current epsilon: 0.79594426\n",
      "episode_length: 19\n",
      "episode reward: 19.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 10395\n",
      "episode: 460\n",
      "running reward: 20.398974345801797\n",
      "current epsilon: 0.79232788\n",
      "episode_length: 20\n",
      "episode reward: 20.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 10617\n",
      "episode: 470\n",
      "running reward: 22.61520593293409\n",
      "current epsilon: 0.78789232\n",
      "episode_length: 18\n",
      "episode reward: 18.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 10891\n",
      "episode: 480\n",
      "running reward: 27.24264677873573\n",
      "current epsilon: 0.7824177999999999\n",
      "episode_length: 41\n",
      "episode reward: 41.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 11264\n",
      "episode: 490\n",
      "running reward: 32.742744797204864\n",
      "current epsilon: 0.77496526\n",
      "episode_length: 16\n",
      "episode reward: 16.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 11613\n",
      "episode: 500\n",
      "running reward: 37.28935585588178\n",
      "current epsilon: 0.76799224\n",
      "episode_length: 100\n",
      "episode reward: 100.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 11861\n",
      "episode: 510\n",
      "running reward: 31.450561421662666\n",
      "current epsilon: 0.7630372\n",
      "episode_length: 19\n",
      "episode reward: 19.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 12279\n",
      "episode: 520\n",
      "running reward: 38.53968826547458\n",
      "current epsilon: 0.75468556\n",
      "episode_length: 92\n",
      "episode reward: 92.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 12647\n",
      "episode: 530\n",
      "running reward: 38.404954720945966\n",
      "current epsilon: 0.7473329200000001\n",
      "episode_length: 58\n",
      "episode reward: 58.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 12929\n",
      "episode: 540\n",
      "running reward: 30.213541455010578\n",
      "current epsilon: 0.74169856\n",
      "episode_length: 15\n",
      "episode reward: 15.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 13192\n",
      "episode: 550\n",
      "running reward: 26.78104246932978\n",
      "current epsilon: 0.7364438200000001\n",
      "episode_length: 17\n",
      "episode reward: 17.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 13435\n",
      "episode: 560\n",
      "running reward: 26.245140152557767\n",
      "current epsilon: 0.73158868\n",
      "episode_length: 25\n",
      "episode reward: 25.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 13650\n",
      "episode: 570\n",
      "running reward: 23.34451241269972\n",
      "current epsilon: 0.72729298\n",
      "episode_length: 13\n",
      "episode reward: 13.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 13974\n",
      "episode: 580\n",
      "running reward: 30.325026774155234\n",
      "current epsilon: 0.72081946\n",
      "episode_length: 46\n",
      "episode reward: 46.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 14441\n",
      "episode: 590\n",
      "running reward: 39.66191504050319\n",
      "current epsilon: 0.7114887999999999\n",
      "episode_length: 15\n",
      "episode reward: 15.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 14828\n",
      "episode: 600\n",
      "running reward: 41.67216428000138\n",
      "current epsilon: 0.70375654\n",
      "episode_length: 98\n",
      "episode reward: 98.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 15297\n",
      "episode: 610\n",
      "running reward: 46.79859732154183\n",
      "current epsilon: 0.69438592\n",
      "episode_length: 66\n",
      "episode reward: 66.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 15669\n",
      "episode: 620\n",
      "running reward: 39.25528234114325\n",
      "current epsilon: 0.68695336\n",
      "episode_length: 20\n",
      "episode reward: 20.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 16162\n",
      "episode: 630\n",
      "running reward: 47.301047420694914\n",
      "current epsilon: 0.67710322\n",
      "episode_length: 44\n",
      "episode reward: 44.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 16565\n",
      "episode: 640\n",
      "running reward: 43.77940827634404\n",
      "current epsilon: 0.6690512799999999\n",
      "episode_length: 16\n",
      "episode reward: 16.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 16977\n",
      "episode: 650\n",
      "running reward: 45.18636494809668\n",
      "current epsilon: 0.66081952\n",
      "episode_length: 36\n",
      "episode reward: 36.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 17428\n",
      "episode: 660\n",
      "running reward: 41.17400656249168\n",
      "current epsilon: 0.65180854\n",
      "episode_length: 47\n",
      "episode reward: 47.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 18087\n",
      "episode: 670\n",
      "running reward: 53.94459720307677\n",
      "current epsilon: 0.6386417200000001\n",
      "episode_length: 74\n",
      "episode reward: 74.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 18525\n",
      "episode: 680\n",
      "running reward: 47.10087262989165\n",
      "current epsilon: 0.62989048\n",
      "episode_length: 97\n",
      "episode reward: 97.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 18945\n",
      "episode: 690\n",
      "running reward: 43.33671260663941\n",
      "current epsilon: 0.6214988800000001\n",
      "episode_length: 51\n",
      "episode reward: 51.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 19371\n",
      "episode: 700\n",
      "running reward: 43.48267808104503\n",
      "current epsilon: 0.6129874\n",
      "episode_length: 18\n",
      "episode reward: 18.0\n",
      "\n",
      "\n",
      "synchronizing target estimator !\n",
      "saving current_model\n",
      "\n",
      "\n",
      "global step: 20085\n",
      "episode: 710\n",
      "running reward: 62.38341769826925\n",
      "current epsilon: 0.59872168\n",
      "episode_length: 85\n",
      "episode reward: 85.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 20711\n",
      "episode: 720\n",
      "running reward: 65.76167889263928\n",
      "current epsilon: 0.5862141999999999\n",
      "episode_length: 79\n",
      "episode reward: 79.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 21183\n",
      "episode: 730\n",
      "running reward: 51.72686134544257\n",
      "current epsilon: 0.5767836399999999\n",
      "episode_length: 61\n",
      "episode reward: 61.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 21821\n",
      "episode: 740\n",
      "running reward: 60.49906118179793\n",
      "current epsilon: 0.5640364\n",
      "episode_length: 81\n",
      "episode reward: 81.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 22612\n",
      "episode: 750\n",
      "running reward: 71.67294322628378\n",
      "current epsilon: 0.54823222\n",
      "episode_length: 60\n",
      "episode reward: 60.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 23127\n",
      "episode: 760\n",
      "running reward: 58.5391627367165\n",
      "current epsilon: 0.5379425200000001\n",
      "episode_length: 64\n",
      "episode reward: 64.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 23629\n",
      "episode: 770\n",
      "running reward: 55.32485054169836\n",
      "current epsilon: 0.5279125600000001\n",
      "episode_length: 111\n",
      "episode reward: 111.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 24333\n",
      "episode: 780\n",
      "running reward: 69.50279964824503\n",
      "current epsilon: 0.5138466399999999\n",
      "episode_length: 124\n",
      "episode reward: 124.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 25222\n",
      "episode: 790\n",
      "running reward: 86.58397072963291\n",
      "current epsilon: 0.49608441999999997\n",
      "episode_length: 155\n",
      "episode reward: 155.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 26059\n",
      "episode: 800\n",
      "running reward: 83.92537756477249\n",
      "current epsilon: 0.47936116000000006\n",
      "episode_length: 16\n",
      "episode reward: 16.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 26934\n",
      "episode: 810\n",
      "running reward: 86.69718070008842\n",
      "current epsilon: 0.46187866\n",
      "episode_length: 119\n",
      "episode reward: 119.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 27958\n",
      "episode: 820\n",
      "running reward: 97.61104914927468\n",
      "current epsilon: 0.44141914000000004\n",
      "episode_length: 83\n",
      "episode reward: 83.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 28592\n",
      "episode: 830\n",
      "running reward: 71.88681609909354\n",
      "current epsilon: 0.42875182\n",
      "episode_length: 71\n",
      "episode reward: 71.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 29497\n",
      "episode: 840\n",
      "running reward: 86.25961940588749\n",
      "current epsilon: 0.41066992\n",
      "episode_length: 93\n",
      "episode reward: 93.0\n",
      "\n",
      "\n",
      "synchronizing target estimator !\n",
      "saving current_model\n",
      "\n",
      "\n",
      "global step: 30327\n",
      "episode: 850\n",
      "running reward: 82.22840853466457\n",
      "current epsilon: 0.3940865200000001\n",
      "episode_length: 114\n",
      "episode reward: 114.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 31395\n",
      "episode: 860\n",
      "running reward: 99.31558612697239\n",
      "current epsilon: 0.37274788000000003\n",
      "episode_length: 153\n",
      "episode reward: 153.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 32702\n",
      "episode: 870\n",
      "running reward: 117.38829877876996\n",
      "current epsilon: 0.34663402\n",
      "episode_length: 107\n",
      "episode reward: 107.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 33974\n",
      "episode: 880\n",
      "running reward: 124.18022231037426\n",
      "current epsilon: 0.32121946\n",
      "episode_length: 158\n",
      "episode reward: 158.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 35384\n",
      "episode: 890\n",
      "running reward: 136.76591542255252\n",
      "current epsilon: 0.29304766000000004\n",
      "episode_length: 159\n",
      "episode reward: 159.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 36865\n",
      "episode: 900\n",
      "running reward: 143.36436057458414\n",
      "current epsilon: 0.2634572799999999\n",
      "episode_length: 155\n",
      "episode reward: 155.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 38249\n",
      "episode: 910\n",
      "running reward: 137.85430390067995\n",
      "current epsilon: 0.23580496000000006\n",
      "episode_length: 77\n",
      "episode reward: 77.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 39827\n",
      "episode: 920\n",
      "running reward: 150.29480769076045\n",
      "current epsilon: 0.20427651999999996\n",
      "episode_length: 149\n",
      "episode reward: 149.0\n",
      "\n",
      "\n",
      "synchronizing target estimator !\n",
      "saving current_model\n",
      "\n",
      "\n",
      "global step: 41393\n",
      "episode: 930\n",
      "running reward: 153.40853694044387\n",
      "current epsilon: 0.17298784\n",
      "episode_length: 127\n",
      "episode reward: 127.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 43002\n",
      "episode: 940\n",
      "running reward: 159.07490186361727\n",
      "current epsilon: 0.14084001999999995\n",
      "episode_length: 183\n",
      "episode reward: 183.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 44580\n",
      "episode: 950\n",
      "running reward: 158.12230619636668\n",
      "current epsilon: 0.10931157999999996\n",
      "episode_length: 150\n",
      "episode reward: 150.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 46186\n",
      "episode: 960\n",
      "running reward: 159.26543057786375\n",
      "current epsilon: 0.0772237\n",
      "episode_length: 149\n",
      "episode reward: 149.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 47871\n",
      "episode: 970\n",
      "running reward: 165.21399640694435\n",
      "current epsilon: 0.04355739999999997\n",
      "episode_length: 167\n",
      "episode reward: 167.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 49459\n",
      "episode: 980\n",
      "running reward: 160.9361646353604\n",
      "current epsilon: 0.011829160000000005\n",
      "episode_length: 147\n",
      "episode reward: 147.0\n",
      "\n",
      "\n",
      "synchronizing target estimator !\n",
      "saving current_model\n",
      "\n",
      "\n",
      "global step: 51051\n",
      "episode: 990\n",
      "running reward: 159.74157431253428\n",
      "current epsilon: 0.001\n",
      "episode_length: 146\n",
      "episode reward: 146.0\n",
      "\n",
      "\n",
      "Current Time: Tue Jul 11 10:46:53 2017\n",
      "Running for: 0:01:31.973061\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 52677\n",
      "episode: 1000\n",
      "running reward: 161.01705425551268\n",
      "current epsilon: 0.001\n",
      "episode_length: 152\n",
      "episode reward: 152.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 54279\n",
      "episode: 1010\n",
      "running reward: 160.53428951710922\n",
      "current epsilon: 0.001\n",
      "episode_length: 151\n",
      "episode reward: 151.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 55936\n",
      "episode: 1020\n",
      "running reward: 163.51481707788747\n",
      "current epsilon: 0.001\n",
      "episode_length: 158\n",
      "episode reward: 158.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 57610\n",
      "episode: 1030\n",
      "running reward: 165.4991805175547\n",
      "current epsilon: 0.001\n",
      "episode_length: 169\n",
      "episode reward: 169.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 59239\n",
      "episode: 1040\n",
      "running reward: 164.29582973418934\n",
      "current epsilon: 0.001\n",
      "episode_length: 168\n",
      "episode reward: 168.0\n",
      "\n",
      "\n",
      "synchronizing target estimator !\n",
      "saving current_model\n",
      "\n",
      "\n",
      "global step: 60887\n",
      "episode: 1050\n",
      "running reward: 164.13795422185237\n",
      "current epsilon: 0.001\n",
      "episode_length: 177\n",
      "episode reward: 177.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 62692\n",
      "episode: 1060\n",
      "running reward: 176.2837152716807\n",
      "current epsilon: 0.001\n",
      "episode_length: 191\n",
      "episode reward: 191.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 64463\n",
      "episode: 1070\n",
      "running reward: 177.4209645957622\n",
      "current epsilon: 0.001\n",
      "episode_length: 181\n",
      "episode reward: 181.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 66311\n",
      "episode: 1080\n",
      "running reward: 183.1801465410877\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 68266\n",
      "episode: 1090\n",
      "running reward: 191.1318645332359\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "synchronizing target estimator !\n",
      "saving current_model\n",
      "\n",
      "\n",
      "global step: 70137\n",
      "episode: 1100\n",
      "running reward: 187.83880901115327\n",
      "current epsilon: 0.001\n",
      "episode_length: 173\n",
      "episode reward: 173.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 71941\n",
      "episode: 1110\n",
      "running reward: 184.3482475942508\n",
      "current epsilon: 0.001\n",
      "episode_length: 176\n",
      "episode reward: 176.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 73904\n",
      "episode: 1120\n",
      "running reward: 191.49408366533197\n",
      "current epsilon: 0.001\n",
      "episode_length: 183\n",
      "episode reward: 183.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 75893\n",
      "episode: 1130\n",
      "running reward: 196.44958526080688\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 77890\n",
      "episode: 1140\n",
      "running reward: 198.63290676403014\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 79888\n",
      "episode: 1150\n",
      "running reward: 199.4276646830108\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "synchronizing target estimator !\n",
      "saving current_model\n",
      "\n",
      "\n",
      "global step: 81708\n",
      "episode: 1160\n",
      "running reward: 186.54700787045812\n",
      "current epsilon: 0.001\n",
      "episode_length: 160\n",
      "episode reward: 160.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 83689\n",
      "episode: 1170\n",
      "running reward: 193.9241316895938\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 85687\n",
      "episode: 1180\n",
      "running reward: 197.6814757152746\n",
      "current epsilon: 0.001\n",
      "episode_length: 198\n",
      "episode reward: 198.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 87665\n",
      "episode: 1190\n",
      "running reward: 197.54234696406803\n",
      "current epsilon: 0.001\n",
      "episode_length: 197\n",
      "episode reward: 197.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 89643\n",
      "episode: 1200\n",
      "running reward: 197.2314693731242\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "synchronizing target estimator !\n",
      "saving current_model\n",
      "\n",
      "\n",
      "global step: 91474\n",
      "episode: 1210\n",
      "running reward: 187.7202401256519\n",
      "current epsilon: 0.001\n",
      "episode_length: 192\n",
      "episode reward: 192.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 93322\n",
      "episode: 1220\n",
      "running reward: 186.62016090320975\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 95126\n",
      "episode: 1230\n",
      "running reward: 180.30552074294218\n",
      "current epsilon: 0.001\n",
      "episode_length: 142\n",
      "episode reward: 142.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 96946\n",
      "episode: 1240\n",
      "running reward: 179.6177066000673\n",
      "current epsilon: 0.001\n",
      "episode_length: 144\n",
      "episode reward: 144.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 98867\n",
      "episode: 1250\n",
      "running reward: 189.43185849865097\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "synchronizing target estimator !\n",
      "saving current_model\n",
      "\n",
      "\n",
      "global step: 100796\n",
      "episode: 1260\n",
      "running reward: 193.33399861705362\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 102784\n",
      "episode: 1270\n",
      "running reward: 196.47570903608985\n",
      "current epsilon: 0.001\n",
      "episode_length: 188\n",
      "episode reward: 188.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 104670\n",
      "episode: 1280\n",
      "running reward: 193.13071153864533\n",
      "current epsilon: 0.001\n",
      "episode_length: 198\n",
      "episode reward: 198.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 106635\n",
      "episode: 1290\n",
      "running reward: 195.87744713469795\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 108563\n",
      "episode: 1300\n",
      "running reward: 194.41391646169672\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "synchronizing target estimator !\n",
      "saving current_model\n",
      "\n",
      "\n",
      "global step: 110526\n",
      "episode: 1310\n",
      "running reward: 195.05525310559617\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 112494\n",
      "episode: 1320\n",
      "running reward: 196.99068959586998\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 114494\n",
      "episode: 1330\n",
      "running reward: 198.95071834251127\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 116494\n",
      "episode: 1340\n",
      "running reward: 199.6341381084413\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 118494\n",
      "episode: 1350\n",
      "running reward: 199.87243184635932\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "synchronizing target estimator !\n",
      "saving current_model\n",
      "\n",
      "\n",
      "global step: 120494\n",
      "episode: 1360\n",
      "running reward: 199.95551973518212\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 122494\n",
      "episode: 1370\n",
      "running reward: 199.98449069064807\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 124442\n",
      "episode: 1380\n",
      "running reward: 195.4384922382082\n",
      "current epsilon: 0.001\n",
      "episode_length: 197\n",
      "episode reward: 197.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 126354\n",
      "episode: 1390\n",
      "running reward: 191.1045827591144\n",
      "current epsilon: 0.001\n",
      "episode_length: 167\n",
      "episode reward: 167.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 128354\n",
      "episode: 1400\n",
      "running reward: 196.8983597924094\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "synchronizing target estimator !\n",
      "saving current_model\n",
      "\n",
      "\n",
      "global step: 130348\n",
      "episode: 1410\n",
      "running reward: 198.6860726372659\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 132348\n",
      "episode: 1420\n",
      "running reward: 199.5418618567572\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 134340\n",
      "episode: 1430\n",
      "running reward: 199.31537710686382\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 136282\n",
      "episode: 1440\n",
      "running reward: 195.72669295756455\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 138162\n",
      "episode: 1450\n",
      "running reward: 191.40908440157529\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "synchronizing target estimator !\n",
      "saving current_model\n",
      "\n",
      "\n",
      "global step: 140028\n",
      "episode: 1460\n",
      "running reward: 186.9156383301105\n",
      "current epsilon: 0.001\n",
      "episode_length: 145\n",
      "episode reward: 145.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 141993\n",
      "episode: 1470\n",
      "running reward: 192.8384591675387\n",
      "current epsilon: 0.001\n",
      "episode_length: 180\n",
      "episode reward: 180.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 143990\n",
      "episode: 1480\n",
      "running reward: 197.30609511382497\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 145960\n",
      "episode: 1490\n",
      "running reward: 196.95271704651077\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 147960\n",
      "episode: 1500\n",
      "running reward: 198.9374781332341\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "global step: 149960\n",
      "episode: 1510\n",
      "running reward: 199.62952153292395\n",
      "current epsilon: 0.001\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "synchronizing target estimator !\n",
      "saving current_model\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-23f6af75f9ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jaidmin/PycharmProjects/pytorch-q-learning/Agent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, start_training, batch_size, env, nr_episodes, decay_until_step, decay_until_value, update_targ_freq)\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0ms_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms2_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mq_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_q_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mr_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0md_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mq_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mepisode_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaidmin/PycharmProjects/pytorch-q-learning/Agent.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, x, targ, actions)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m         \u001b[0;31m#for param in self.estimator.parameters():\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;31m#    param.grad.data.clamp_(-1, 1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaidmin/anaconda2/envs/pytorch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munless\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mvolatile\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \"\"\"\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaidmin/anaconda2/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0mgrad_variables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mgrad_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_variables\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaidmin/anaconda2/envs/pytorch/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, user_create_graph)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgrad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaidmin/anaconda2/envs/pytorch/lib/python3.6/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fallthrough_methods\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(100,128,env,100000,50000,0.001,10000)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
