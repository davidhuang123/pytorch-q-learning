{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import Agent; reload(Agent)\n",
    "import Estimator; reload(Estimator)\n",
    "\n",
    "from Agent import Agent\n",
    "\n",
    "from Estimator import LinearEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearEstimator (\n",
       "  (dense_1): Linear (4 -> 64)\n",
       "  (out): Linear (64 -> 2)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_pred = LinearEstimator(2)\n",
    "linear_targ = LinearEstimator(2)\n",
    "linear_pred.cuda()\n",
    "linear_targ.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "agent = Agent(2,linear_pred,linear_targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-11 10:18:08,144] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "_env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from Environment import Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = Environment(_env,1,False,[0,1],False,-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training loop at: Tue Jul 11 10:18:49 2017\n",
      "populating replay buffer... \n",
      "replay buffer populated with 100 transitions, learning begins...\n",
      "global step: 246\n",
      "episode: 10\n",
      "running reward: 16.554441637300002\n",
      "current epsilon: 0.97795\n",
      "episode_length: 13\n",
      "episode reward: 13.0\n",
      "\n",
      "\n",
      "global step: 482\n",
      "episode: 20\n",
      "running reward: 23.67113736932026\n",
      "current epsilon: 0.95671\n",
      "episode_length: 67\n",
      "episode reward: 67.0\n",
      "\n",
      "\n",
      "global step: 804\n",
      "episode: 30\n",
      "running reward: 29.832089911027413\n",
      "current epsilon: 0.9277299999999999\n",
      "episode_length: 18\n",
      "episode reward: 18.0\n",
      "\n",
      "\n",
      "global step: 1111\n",
      "episode: 40\n",
      "running reward: 31.30721442059999\n",
      "current epsilon: 0.9001\n",
      "episode_length: 76\n",
      "episode reward: 76.0\n",
      "\n",
      "\n",
      "global step: 1398\n",
      "episode: 50\n",
      "running reward: 30.212886718951033\n",
      "current epsilon: 0.87427\n",
      "episode_length: 55\n",
      "episode reward: 55.0\n",
      "\n",
      "\n",
      "global step: 1633\n",
      "episode: 60\n",
      "running reward: 26.07531817068186\n",
      "current epsilon: 0.85312\n",
      "episode_length: 31\n",
      "episode reward: 31.0\n",
      "\n",
      "\n",
      "global step: 1830\n",
      "episode: 70\n",
      "running reward: 21.46471621526454\n",
      "current epsilon: 0.83539\n",
      "episode_length: 19\n",
      "episode reward: 19.0\n",
      "\n",
      "\n",
      "global step: 2209\n",
      "episode: 80\n",
      "running reward: 30.906082511027623\n",
      "current epsilon: 0.80128\n",
      "episode_length: 41\n",
      "episode reward: 41.0\n",
      "\n",
      "\n",
      "global step: 2520\n",
      "episode: 90\n",
      "running reward: 32.162989761647005\n",
      "current epsilon: 0.77329\n",
      "episode_length: 29\n",
      "episode reward: 29.0\n",
      "\n",
      "\n",
      "global step: 2766\n",
      "episode: 100\n",
      "running reward: 27.076417965243348\n",
      "current epsilon: 0.75115\n",
      "episode_length: 19\n",
      "episode reward: 19.0\n",
      "synchronizing target estimator !\n",
      "saving zwischenstand\n",
      "\n",
      "\n",
      "global step: 3106\n",
      "episode: 110\n",
      "running reward: 31.953838656816675\n",
      "current epsilon: 0.72055\n",
      "episode_length: 16\n",
      "episode reward: 16.0\n",
      "\n",
      "\n",
      "global step: 3370\n",
      "episode: 120\n",
      "running reward: 28.136387379365924\n",
      "current epsilon: 0.69679\n",
      "episode_length: 26\n",
      "episode reward: 26.0\n",
      "\n",
      "\n",
      "global step: 3722\n",
      "episode: 130\n",
      "running reward: 33.68651690538664\n",
      "current epsilon: 0.66511\n",
      "episode_length: 18\n",
      "episode reward: 18.0\n",
      "\n",
      "\n",
      "global step: 4109\n",
      "episode: 140\n",
      "running reward: 36.22133959227249\n",
      "current epsilon: 0.63028\n",
      "episode_length: 19\n",
      "episode reward: 19.0\n",
      "\n",
      "\n",
      "global step: 4401\n",
      "episode: 150\n",
      "running reward: 31.42361269646594\n",
      "current epsilon: 0.604\n",
      "episode_length: 38\n",
      "episode reward: 38.0\n",
      "\n",
      "\n",
      "global step: 4744\n",
      "episode: 160\n",
      "running reward: 33.75678661641031\n",
      "current epsilon: 0.5731299999999999\n",
      "episode_length: 44\n",
      "episode reward: 44.0\n",
      "\n",
      "\n",
      "global step: 5259\n",
      "episode: 170\n",
      "running reward: 47.02964065879852\n",
      "current epsilon: 0.52678\n",
      "episode_length: 32\n",
      "episode reward: 32.0\n",
      "\n",
      "\n",
      "global step: 5685\n",
      "episode: 180\n",
      "running reward: 43.26532511677342\n",
      "current epsilon: 0.48844\n",
      "episode_length: 41\n",
      "episode reward: 41.0\n",
      "\n",
      "\n",
      "global step: 6281\n",
      "episode: 190\n",
      "running reward: 54.58728704613591\n",
      "current epsilon: 0.43479999999999996\n",
      "episode_length: 71\n",
      "episode reward: 71.0\n",
      "\n",
      "\n",
      "global step: 6809\n",
      "episode: 200\n",
      "running reward: 52.28674571783762\n",
      "current epsilon: 0.38728000000000007\n",
      "episode_length: 39\n",
      "episode reward: 39.0\n",
      "synchronizing target estimator !\n",
      "saving zwischenstand\n",
      "\n",
      "\n",
      "global step: 7217\n",
      "episode: 210\n",
      "running reward: 44.345110372400974\n",
      "current epsilon: 0.35056\n",
      "episode_length: 45\n",
      "episode reward: 45.0\n",
      "\n",
      "\n",
      "global step: 7806\n",
      "episode: 220\n",
      "running reward: 57.87670152961111\n",
      "current epsilon: 0.29755\n",
      "episode_length: 151\n",
      "episode reward: 151.0\n",
      "\n",
      "\n",
      "global step: 9080\n",
      "episode: 230\n",
      "running reward: 103.33139839867809\n",
      "current epsilon: 0.18289\n",
      "episode_length: 136\n",
      "episode reward: 136.0\n",
      "\n",
      "\n",
      "global step: 10818\n",
      "episode: 240\n",
      "running reward: 149.8440456950027\n",
      "current epsilon: 0.1\n",
      "episode_length: 161\n",
      "episode reward: 161.0\n",
      "\n",
      "\n",
      "global step: 12513\n",
      "episode: 250\n",
      "running reward: 160.7919901254067\n",
      "current epsilon: 0.1\n",
      "episode_length: 55\n",
      "episode reward: 55.0\n",
      "\n",
      "\n",
      "global step: 13856\n",
      "episode: 260\n",
      "running reward: 140.10273269320146\n",
      "current epsilon: 0.1\n",
      "episode_length: 79\n",
      "episode reward: 79.0\n",
      "\n",
      "\n",
      "global step: 15377\n",
      "episode: 270\n",
      "running reward: 150.5503511194128\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "global step: 16804\n",
      "episode: 280\n",
      "running reward: 145.71685951252417\n",
      "current epsilon: 0.1\n",
      "episode_length: 142\n",
      "episode reward: 142.0\n",
      "\n",
      "\n",
      "global step: 18040\n",
      "episode: 290\n",
      "running reward: 131.9289922223978\n",
      "current epsilon: 0.1\n",
      "episode_length: 118\n",
      "episode reward: 118.0\n",
      "\n",
      "\n",
      "global step: 19316\n",
      "episode: 300\n",
      "running reward: 130.3288713356707\n",
      "current epsilon: 0.1\n",
      "episode_length: 113\n",
      "episode reward: 113.0\n",
      "synchronizing target estimator !\n",
      "saving zwischenstand\n",
      "\n",
      "\n",
      "global step: 20814\n",
      "episode: 310\n",
      "running reward: 148.62096758691527\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "global step: 22814\n",
      "episode: 320\n",
      "running reward: 182.0852391243583\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "global step: 24814\n",
      "episode: 330\n",
      "running reward: 193.75350912311677\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "global step: 26814\n",
      "episode: 340\n",
      "running reward: 197.8219833049495\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "global step: 28814\n",
      "episode: 350\n",
      "running reward: 199.24057253625807\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "global step: 30814\n",
      "episode: 360\n",
      "running reward: 199.73520401657342\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "global step: 32812\n",
      "episode: 370\n",
      "running reward: 199.7895733495541\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "global step: 34779\n",
      "episode: 380\n",
      "running reward: 197.7614987637671\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "global step: 36691\n",
      "episode: 390\n",
      "running reward: 191.98120784078844\n",
      "current epsilon: 0.1\n",
      "episode_length: 198\n",
      "episode reward: 198.0\n",
      "\n",
      "\n",
      "global step: 38641\n",
      "episode: 400\n",
      "running reward: 193.98051173944003\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "synchronizing target estimator !\n",
      "saving zwischenstand\n",
      "\n",
      "\n",
      "global step: 40600\n",
      "episode: 410\n",
      "running reward: 194.9632642231077\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "global step: 42570\n",
      "episode: 420\n",
      "running reward: 196.26190848411736\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "global step: 44512\n",
      "episode: 430\n",
      "running reward: 194.893669989191\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "global step: 46495\n",
      "episode: 440\n",
      "running reward: 196.86053281719532\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "global step: 48477\n",
      "episode: 450\n",
      "running reward: 198.05420474645456\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "global step: 50454\n",
      "episode: 460\n",
      "running reward: 197.62409904623985\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "global step: 52332\n",
      "episode: 470\n",
      "running reward: 190.75654044591084\n",
      "current epsilon: 0.1\n",
      "episode_length: 179\n",
      "episode reward: 179.0\n",
      "\n",
      "\n",
      "global step: 54328\n",
      "episode: 480\n",
      "running reward: 196.5644285415528\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n",
      "global step: 56241\n",
      "episode: 490\n",
      "running reward: 193.45691692101659\n",
      "current epsilon: 0.1\n",
      "episode_length: 195\n",
      "episode reward: 195.0\n",
      "\n",
      "\n",
      "global step: 58157\n",
      "episode: 500\n",
      "running reward: 192.25531450457538\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "synchronizing target estimator !\n",
      "saving zwischenstand\n",
      "\n",
      "\n",
      "global step: 60106\n",
      "episode: 510\n",
      "running reward: 194.68962996159024\n",
      "current epsilon: 0.1\n",
      "episode_length: 200\n",
      "episode reward: 200.0\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-d4d715a62c07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/jaidmin/PycharmProjects/pytorch-q-learning/Agent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, start_training, batch_size, env, nr_episodes, decay_until_step, decay_until_value)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m                 \u001b[0meps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_epsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_until_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_until_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 148\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_e_greedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    149\u001b[0m                 \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m                 \u001b[0mtotal_steps\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaidmin/PycharmProjects/pytorch-q-learning/Agent.py\u001b[0m in \u001b[0;36mchoose_e_greedy_action\u001b[0;34m(self, state, epsilon)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoose_greedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose_greedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaidmin/PycharmProjects/pytorch-q-learning/Agent.py\u001b[0m in \u001b[0;36mchoose_greedy_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose_greedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaidmin/PycharmProjects/pytorch-q-learning/Agent.py\u001b[0m in \u001b[0;36mpredict_q_values\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \"\"\"\n\u001b[1;32m     50\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mchoose_e_greedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaidmin/PycharmProjects/pytorch-q-learning/Estimator.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jaidmin/anaconda2/envs/pytorch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(100,128,env,100000,10000,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_1 = env.reset()\n",
    "test_1 = Variable(torch.unsqueeze(torch.from_numpy(test_1).float().cuda(),0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "linear_pred(test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = Variable"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
